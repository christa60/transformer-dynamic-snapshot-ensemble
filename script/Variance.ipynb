{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/temp_work/ch229121/GlueTraining/setting-12/'\n",
    "workdir = '/home/ch229121/Projects/Thyme/thyme_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def logit2prob(logit):\n",
    "  odds = math.exp(logit)\n",
    "  prob = odds / (1 + odds)\n",
    "  return prob\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_probabilities(slurm_id):\n",
    "    return dict_probabilities[slurm_id]\n",
    "    \n",
    "def find_probabilities_for_sampling(probabilities, sampling_no, num_initializations_per_split):\n",
    "    return probabilities[sampling_no * num_initializations_per_split:\n",
    "        (sampling_no+1) * num_initializations_per_split]\n",
    "\n",
    "def find_probabilities_for_optimization(probabilities, sampling_no, num_initializations_per_split):\n",
    "    return probabilities[sampling_no::num_initializations_per_split]\n",
    "\n",
    "def load_probabilities_and_get_first_term(slurm_id, num_initializations_per_split, reverse=False):\n",
    "    '''Variance due to optimization'''\n",
    "    \n",
    "    probabilities = load_probabilities(slurm_id)\n",
    "\n",
    "    num_samplings = probabilities.shape[0] // num_initializations_per_split\n",
    "    individual_variances = []\n",
    "    for sampling_no in range(num_samplings):\n",
    "        if not reverse:\n",
    "            probabilities_for_this_sampling = find_probabilities_for_sampling(probabilities, sampling_no, num_initializations_per_split)\n",
    "        else:\n",
    "            probabilities_for_this_sampling = find_probabilities_for_optimization(probabilities, sampling_no, num_initializations_per_split)\n",
    "        individual_variance = calculate_variance(probabilities_for_this_sampling)\n",
    "        individual_variances.append(individual_variance)\n",
    "\n",
    "    first_term = np.mean(np.array(individual_variances))\n",
    "    return first_term\n",
    "\n",
    "def load_probabilities_and_get_second_term(slurm_id, num_initializations_per_split, reverse=False):\n",
    "    '''Variance due to sampling'''\n",
    "\n",
    "    probabilities = load_probabilities(slurm_id)\n",
    "\n",
    "    num_samplings = probabilities.shape[0] // num_initializations_per_split\n",
    "\n",
    "    expected_probabilities_shape = list(probabilities.shape)\n",
    "    expected_probabilities_shape[0] = num_samplings\n",
    "\n",
    "    expected_probabilities = np.zeros(expected_probabilities_shape)\n",
    "\n",
    "    for sampling_no in range(num_samplings):\n",
    "        if not reverse:\n",
    "            probabilities_for_this_sampling = find_probabilities_for_sampling(probabilities, sampling_no, num_initializations_per_split)\n",
    "        else:\n",
    "            probabilities_for_this_sampling = find_probabilities_for_optimization(probabilities, sampling_no, num_initializations_per_split)\n",
    "        expected_probabilities[sampling_no] = np.mean(probabilities_for_this_sampling, 0)\n",
    "\n",
    "    second_term = calculate_variance(expected_probabilities)\n",
    "\n",
    "    return second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(bitmaps):\n",
    "    mean = np.mean(bitmaps, 0)\n",
    "    return np.mean((bitmaps - np.expand_dims(mean, axis=0)) ** 2)\n",
    "\n",
    "\n",
    "def calculate_bias(probabilities, test_y_onehot):\n",
    "    mean = np.mean(probabilities, 0)\n",
    "    return np.mean((mean - test_y_onehot) ** 2)\n",
    "\n",
    "def calculate_losses(probabilities, test_y_onehot):\n",
    "    print(probabilities.shape, test_y_onehot.shape)\n",
    "    pred = np.argmax(probabilities, axis=1)\n",
    "    target = np.argmax(test_y_onehot, axis=1)\n",
    "    losses = np.mean(pred != target, axis=0)\n",
    "    return losses\n",
    "    \n",
    "def load_probabilities_and_get_variances(slurm_id):\n",
    "    probabilities = load_probabilities(slurm_id)\n",
    "    original_variance = calculate_variance(probabilities)\n",
    "    return original_variance\n",
    "\n",
    "def load_probabilities_and_get_bias(slurm_id):\n",
    "    probabilities = load_probabilities(slurm_id)\n",
    "    original_bias = calculate_bias(probabilities, test_y_onehot)\n",
    "    return original_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/programs/local/anaconda/2019.10/3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labelfile = open(\"/home/ch229121/Projects/Thyme/thyme_data/test_label.txt\", \"r\")\n",
    "labels = labelfile.read().splitlines()\n",
    "labelfile.close()\n",
    "\n",
    "integer_encoded = np.array(labels)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoder = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_onehot_all = onehot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_index = np.where(integer_encoded!='6')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_onehot_no_none = onehot_encoder[keep_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 208891, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_probabilities_all = defaultdict()\n",
    "# slurm_id = 'PubmedBERTbase-MimicBig-EntityBERT'\n",
    "# slurm_id = 'roberta-base'\n",
    "seeds = [42, 52, 62, 72, 82]\n",
    "splits = ['split_r42', 'split_r52', 'split_r62', 'split_r72', 'split_r82']\n",
    "for slurm_id in ['PubmedBERTbase-MimicBig-EntityBERT', 'roberta-base']:\n",
    "    probabilities = []\n",
    "    for split in splits:\n",
    "        for seed in seeds:\n",
    "            df = pd.read_csv(datadir+f'thyme_{slurm_id}_lr.4e-5_linear_epoch.3_seed.{seed}_{split}/predict_results_prob_None.txt', header=0, index_col=0, delimiter='\\t')\n",
    "            df['prediction'] = df.prediction.apply(lambda x: softmax([logit2prob(float(i)) for i in x.split(',')]))\n",
    "            probabilities.append(np.stack(df.prediction.values))\n",
    "    probabilities = np.array(probabilities)\n",
    "    print(probabilities.shape)\n",
    "\n",
    "    dict_probabilities_all[slurm_id] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_probabilities_no_none = defaultdict()\n",
    "for slurm_id in ['PubmedBERTbase-MimicBig-EntityBERT', 'roberta-base']:\n",
    "    dict_probabilities_no_none[slurm_id] = dict_probabilities_all[slurm_id][:, keep_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate bias and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "num_initializations_per_split = 5\n",
    "dict_probabilities = dict_probabilities_no_none\n",
    "test_y_onehot = test_y_onehot_no_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: 0.08128837954355322\n",
      "variance: 0.000166435612810771\n",
      "variance_optimization: 0.0001434072005010661\n",
      "variance_sampling: 2.3028412309705126e-05\n",
      "Phase-1\n"
     ]
    }
   ],
   "source": [
    "slurm_id = 'roberta-base'\n",
    "bias = load_probabilities_and_get_bias(slurm_id, )\n",
    "variance = load_probabilities_and_get_variances(slurm_id)\n",
    "var_opt = load_probabilities_and_get_first_term(slurm_id, num_initializations_per_split)\n",
    "var_samp = load_probabilities_and_get_second_term(slurm_id, num_initializations_per_split)\n",
    "print(f'bias: {bias}')\n",
    "print(f'variance: {variance}')\n",
    "print(f'variance_optimization: {var_opt}')\n",
    "print(f'variance_sampling: {var_samp}')\n",
    "if var_opt > var_samp:\n",
    "    print('Phase-1')\n",
    "elif var_opt < var_samp:\n",
    "    print('Phase-3')\n",
    "else:\n",
    "    print('Phase-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: 0.07965660418517094\n",
      "variance: 8.672195372488624e-05\n",
      "variance_optimization: 6.512269207249076e-05\n",
      "variance_sampling: 2.159926165239544e-05\n",
      "Phase-1\n"
     ]
    }
   ],
   "source": [
    "slurm_id = 'PubmedBERTbase-MimicBig-EntityBERT'\n",
    "bias = load_probabilities_and_get_bias(slurm_id, )\n",
    "variance = load_probabilities_and_get_variances(slurm_id)\n",
    "var_opt = load_probabilities_and_get_first_term(slurm_id, num_initializations_per_split)\n",
    "var_samp = load_probabilities_and_get_second_term(slurm_id, num_initializations_per_split)\n",
    "print(f'bias: {bias}')\n",
    "print(f'variance: {variance}')\n",
    "print(f'variance_optimization: {var_opt}')\n",
    "print(f'variance_sampling: {var_samp}')\n",
    "if var_opt > var_samp:\n",
    "    print('Phase-1')\n",
    "elif var_opt < var_samp:\n",
    "    print('Phase-3')\n",
    "else:\n",
    "    print('Phase-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0836389615118786 0.00026890083667728505\n",
      "0.08139853390668515 0.00015536677771869559\n",
      "0.08151385272631272 0.00010384289266284722\n",
      "0.08036358371345685 6.610834769145817e-05\n",
      "0.07970514960886459 5.977545987179058e-05\n"
     ]
    }
   ],
   "source": [
    "for sampling_no in [0, 1, 2, 3, 4]:\n",
    "    probabilities_for_this_sampling = dict_probabilities_no_none['roberta-base'][sampling_no::5]\n",
    "    bias = calculate_bias(probabilities_for_this_sampling, test_y_onehot_no_none)\n",
    "    var = calculate_variance(probabilities_for_this_sampling)\n",
    "    print(bias, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07945230026220991 7.742348063477951e-05\n",
      "0.08041242775108372 6.501960345408497e-05\n",
      "0.07974000258495775 7.146146369743825e-05\n",
      "0.07930017185001845 6.754454443238603e-05\n",
      "0.07946107497696271 6.920417702795085e-05\n"
     ]
    }
   ],
   "source": [
    "for sampling_no in [0, 1, 2, 3, 4]:\n",
    "    probabilities_for_this_sampling = dict_probabilities_no_none['PubmedBERTbase-MimicBig-EntityBERT'][sampling_no::5]\n",
    "    bias = calculate_bias(probabilities_for_this_sampling, test_y_onehot_no_none)\n",
    "    var = calculate_variance(probabilities_for_this_sampling)\n",
    "    print(bias, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from bias-variance github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(bitmaps):\n",
    "    mean = np.mean(bitmaps, 0)\n",
    "    return np.mean((bitmaps - np.expand_dims(mean, axis=0)) ** 2)\n",
    "\n",
    "\n",
    "def get_variance(slurm_id, num_hidden,inter):\n",
    "    '''\n",
    "    Returns the variance for a slurm id (corresponding to an experiment) and a hidden size.\n",
    "    '''\n",
    "    probabilities = load_probabilities(slurm_id, num_hidden,inter)\n",
    "    return calculate_variance(probabilities)\n",
    "\n",
    "\n",
    "def calculate_bias(probabilities, test_y_onehot):\n",
    "    mean = np.mean(probabilities, 0)\n",
    "    return np.mean((mean - test_y_onehot) ** 2)\n",
    "\n",
    "\n",
    "def calculate_losses(probabilities, test_y_onehot):\n",
    "    print(probabilities.shape, test_y_onehot.shape)\n",
    "    pred = np.argmax(probabilities, axis=1)\n",
    "    target = np.argmax(test_y_onehot, axis=1)\n",
    "    losses = np.mean(pred != target, axis=1)\n",
    "    return losses\n",
    "\n",
    "\n",
    "def get_bias(slurm_id, num_hidden, inter):\n",
    "    '''\n",
    "    Returns the variance for a slurm id (corresponding to an experiment) and a hidden size.\n",
    "    '''\n",
    "    test_y_onehot = get_test_y_onehot()\n",
    "    probabilities = load_probabilities(slurm_id, num_hidden,inter)\n",
    "    return calculate_bias(probabilities, test_y_onehot)\n",
    "\n",
    "\n",
    "def load_probabilities_and_get_variances(slurm_id, hidden_arr,inter=0, num_bootstrap=10000):\n",
    "    '''\n",
    "    Loads saved probabilities, calculates differences using bootstrapping from\n",
    "    the value of variance computed using all the seeds and saves those diffs.\n",
    "    Prerequisite: Probabilities should be saved earlier using the\n",
    "    save_probabilities function with dimension (num_seeds, num_test_examples,\n",
    "    probabilities_for_each_example), eg. (50, 10000, 10) for 50 seeds for MNIST.\n",
    "    '''\n",
    "    for num_hidden in hidden_arr:\n",
    "        probabilities = load_probabilities(slurm_id, num_hidden, inter)\n",
    "        original_variance = calculate_variance(probabilities)\n",
    "\n",
    "        diffs = []\n",
    "        for i in range(num_bootstrap):\n",
    "            indices = np.random.choice(50, 50, replace=True)\n",
    "            if slurm_id == 195683:\n",
    "                indices = np.random.choice(28, 28, replace=True)\n",
    "            elif slurm_id == 195684:\n",
    "                indices = np.random.choice(43, 43, replace=True)\n",
    "            bootstrap_probabilities = probabilities[indices]\n",
    "            bootstrap_variance = calculate_variance(bootstrap_probabilities)\n",
    "            diff_variance = (bootstrap_variance - original_variance)\n",
    "            diffs.append(diff_variance)\n",
    "\n",
    "        save_variance_diffs(slurm_id, num_hidden, diffs)\n",
    "\n",
    "\n",
    "def find_probabilities_for_sampling(probabilities, sampling_no, num_initializations_per_split):\n",
    "    return probabilities[sampling_no * num_initializations_per_split:\n",
    "        (sampling_no+1) * num_initializations_per_split]\n",
    "\n",
    "def find_probabilities_for_optimization(probabilities, sampling_no, num_initializations_per_split):\n",
    "    return probabilities[sampling_no::num_initializations_per_split]\n",
    "\n",
    "def load_probabilities_and_get_first_term(slurm_id, hidden_arr, num_initializations_per_split, inter=0, reverse=False):\n",
    "    first_terms = []\n",
    "    for num_hidden in hidden_arr:\n",
    "        probabilities = load_probabilities(slurm_id, num_hidden, inter)\n",
    "\n",
    "        num_samplings = probabilities.shape[0] // num_initializations_per_split\n",
    "        individual_variances = []\n",
    "        for sampling_no in range(num_samplings):\n",
    "            if not reverse:\n",
    "                probabilities_for_this_sampling = find_probabilities_for_sampling(probabilities, sampling_no, num_initializations_per_split)\n",
    "            else:\n",
    "                probabilities_for_this_sampling = find_probabilities_for_optimization(probabilities, sampling_no, num_initializations_per_split)\n",
    "            individual_variance = calculate_variance(probabilities_for_this_sampling)\n",
    "            individual_variances.append(individual_variance)\n",
    "\n",
    "        first_terms.append(np.mean(np.array(individual_variances)))\n",
    "    return first_terms\n",
    "\n",
    "\n",
    "def load_probabilities_and_get_second_term(slurm_id, hidden_arr, num_initializations_per_split, inter=0, reverse=False):\n",
    "    second_terms = []\n",
    "    for num_hidden in hidden_arr:\n",
    "        probabilities = load_probabilities(slurm_id, num_hidden, inter)\n",
    "\n",
    "        num_samplings = probabilities.shape[0] // num_initializations_per_split\n",
    "\n",
    "        expected_probabilities_shape = list(probabilities.shape)\n",
    "        expected_probabilities_shape[0] = num_samplings\n",
    "\n",
    "        expected_probabilities = np.zeros(expected_probabilities_shape)\n",
    "\n",
    "        for sampling_no in range(num_samplings):\n",
    "            if not reverse:\n",
    "                probabilities_for_this_sampling = find_probabilities_for_sampling(probabilities, sampling_no, num_initializations_per_split)\n",
    "            else:\n",
    "                probabilities_for_this_sampling = find_probabilities_for_optimization(probabilities, sampling_no, num_initializations_per_split)\n",
    "            expected_probabilities[sampling_no] = np.mean(probabilities_for_this_sampling, 0)\n",
    "\n",
    "        second_terms.append(calculate_variance(expected_probabilities))\n",
    "\n",
    "    return second_terms\n",
    "\n",
    "\n",
    "def load_probabilities_and_get_biases(slurm_id, hidden_arr, inter =0, num_bootstrap=10000):\n",
    "    test_y_onehot = get_test_y_onehot()\n",
    "    for num_hidden in hidden_arr:\n",
    "        probabilities = load_probabilities(slurm_id, num_hidden, inter)\n",
    "        original_variance = calculate_bias(probabilities, test_y_onehot)\n",
    "\n",
    "        diffs = []\n",
    "        for i in range(num_bootstrap):\n",
    "            indices = np.random.choice(50, 50, replace=True)\n",
    "            if slurm_id == 195683:\n",
    "                indices = np.random.choice(28, 28, replace=True)\n",
    "            elif slurm_id == 195684:\n",
    "                indices = np.random.choice(43, 43, replace=True)\n",
    "            bootstrap_probabilities = probabilities[indices]\n",
    "            bootstrap_variance = calculate_bias(bootstrap_probabilities, test_y_onehot)\n",
    "            diff_variance = (bootstrap_variance - original_variance)\n",
    "            diffs.append(diff_variance)\n",
    "\n",
    "        save_bias_diffs(slurm_id, num_hidden, diffs)\n",
    "\n",
    "\n",
    "def load_probabilities_and_get_losses_and_std(slurm_id, hidden_arr,inter =0):\n",
    "    test_y_onehot = get_test_y_onehot()\n",
    "    average_losses, stds = [], []\n",
    "    for num_hidden in hidden_arr:\n",
    "        probabilities = load_probabilities(slurm_id, num_hidden, inter)\n",
    "        losses = calculate_losses(probabilities, test_y_onehot)\n",
    "\n",
    "        average_loss = np.mean(losses)\n",
    "        std = np.std(losses)/math.sqrt(len(losses))\n",
    "\n",
    "        average_losses.append(average_loss)\n",
    "        stds.append(std)\n",
    "\n",
    "    return average_losses, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_onehot = np.array([[1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 1., 0.]])\n",
    "probabilities = np.array([[0.3, 0.6, 0.1], [0.0, 0.8, 0.2], [0.3, 0.7, 0.0], [0.0, 0.0, 1.0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(bitmaps):\n",
    "    mean = np.mean(bitmaps, 0)\n",
    "    return np.mean((bitmaps - np.expand_dims(mean, axis=0)) ** 2)\n",
    "\n",
    "\n",
    "def calculate_bias(probabilities, test_y_onehot):\n",
    "    mean = np.mean(probabilities, 0)\n",
    "    return np.mean((mean - test_y_onehot) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7225  , 0.275625, 0.105625],\n",
       "       [0.0225  , 0.225625, 0.105625],\n",
       "       [0.0225  , 0.225625, 0.105625],\n",
       "       [0.0225  , 0.225625, 0.105625]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(probabilities, 0) - test_y_onehot) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18041666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.mean(probabilities, 0) - test_y_onehot) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09208333333333334 0.18041666666666667\n"
     ]
    }
   ],
   "source": [
    "var = calculate_variance(probabilities)\n",
    "bias = calculate_bias(probabilities, test_y_onehot)\n",
    "print(var, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0693e-05, 3.1369e+27, 7.0800e+31],\n",
      "        [3.1095e-18, 1.8590e+34, 7.7767e+31],\n",
      "        [7.1536e+22, 3.3803e-18, 1.9421e+31],\n",
      "        [2.7491e+20, 6.1949e-04, 1.3527e-08]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 0.]])\n",
      "[[1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "MNIST_TEST_SIZE = 4\n",
    "NUM_MNIST_CLASSES = 3\n",
    "test_y_onehot = torch.FloatTensor(MNIST_TEST_SIZE, NUM_MNIST_CLASSES)\n",
    "print(test_y_onehot)\n",
    "test_y_onehot.zero_()\n",
    "print(test_y_onehot)\n",
    "index = torch.tensor([[0, 1, 2], [0, 0, 0], [0, 1, 2], [0, 1, 0]])\n",
    "test_y_onehot.scatter_(1, index, 1)\n",
    "print(test_y_onehot)\n",
    "test_y_onehot = test_y_onehot.cpu().numpy()\n",
    "print(test_y_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
